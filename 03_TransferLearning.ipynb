{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./lib/Image.jl\")\n",
    "include(\"./lib/TorchVision.jl\")\n",
    "using .TorchVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <torch._C.Generator object at 0x7f4b9e7dcc30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "# 乱数初期化\n",
    "## Random.seed!([rng=GLOBAL_RNG], seed) -> rng\n",
    "## Random.seed!([rng=GLOBAL_RNG]) -> rng\n",
    "### `!`付きの関数は第一引数の値を破壊的に変更する\n",
    "Random.seed!(1234)\n",
    "\n",
    "# PyTorchの乱数初期化\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#3 (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "# 訓練用、予測用の画像変換関数を作成する関数\n",
    "## () -> ((PyObject, String) -> Array{Float32,3})\n",
    "make_transformer_for_learning() = begin\n",
    "    resize = 224\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    transform = Dict(\n",
    "        \"train\" => make_transformer(\n",
    "            transforms.RandomResizedCrop(resize; scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ),\n",
    "        \"val\" => make_transformer(\n",
    "            transforms.Resize(resize),\n",
    "            transforms.CenterCrop(resize),\n",
    "            transforms.Normalize(mean, std)\n",
    "        )\n",
    "    )\n",
    "    return (image::PyObject; phase::String=\"train\") -> transform[phase](image)\n",
    "end\n",
    "\n",
    "image_transform_vgg16 = make_transformer_for_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585-element Array{String,1}:\n",
       " \"./dataset/train/hedgehog/118523311_32345c36a2.jpg\"    \n",
       " \"./dataset/train/hedgehog/1241612498_7ab4277d10.jpg\"   \n",
       " \"./dataset/train/hedgehog/126009980_9004803c9e.jpg\"    \n",
       " \"./dataset/train/hedgehog/1274493397_88388552d8.jpg\"   \n",
       " \"./dataset/train/hedgehog/127772208_f65a074ed5.jpg\"    \n",
       " \"./dataset/train/hedgehog/1295991716_4ad47dae66.jpg\"   \n",
       " \"./dataset/train/hedgehog/1296287640_19d39d5b1e.jpg\"   \n",
       " \"./dataset/train/hedgehog/1322807353_6eec9596b3.jpg\"   \n",
       " \"./dataset/train/hedgehog/150464690_e33dd1938d.jpg\"    \n",
       " \"./dataset/train/hedgehog/159959475_fb41beb469.jpg\"    \n",
       " \"./dataset/train/hedgehog/163878245_fd30b5169b.jpg\"    \n",
       " \"./dataset/train/hedgehog/17404099_32851ad117.jpg\"     \n",
       " \"./dataset/train/hedgehog/176380875_d2ad991223.jpg\"    \n",
       " ⋮                                                      \n",
       " \"./dataset/train/porcupine/PA210066.JPG\"               \n",
       " \"./dataset/train/porcupine/porcupine_sc108.jpg\"        \n",
       " \"./dataset/train/porcupine/porcupine_sud_america.jpg\"  \n",
       " \"./dataset/train/porcupine/puerco_espin_comun.jpg\"     \n",
       " \"./dataset/train/porcupine/somekinda-porcupine-big.jpg\"\n",
       " \"./dataset/train/porcupine/speaking-porcupine2.jpg\"    \n",
       " \"./dataset/train/porcupine/specCRFS_02Istrice.jpg\"     \n",
       " \"./dataset/train/porcupine/tn-porcoespinho.jpg\"        \n",
       " \"./dataset/train/porcupine/tn_porcupine_jpg.jpg\"       \n",
       " \"./dataset/train/porcupine/yun_3882.jpg\"               \n",
       " \"./dataset/train/porcupine/zoo+070.jpg\"                \n",
       " \"./dataset/train/porcupine/zporcupine.jpg\"             "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシの画像へのファイルパスのリスト作成\n",
    "make_dataset_list(phase::String=\"train\") = begin\n",
    "    hedgehogs = map(\n",
    "        path -> \"./dataset/$(phase)/hedgehog/$(path)\",\n",
    "        readdir(\"./dataset/$(phase)/hedgehog/\")\n",
    "    )\n",
    "    porcupines = map(\n",
    "        path -> \"./dataset/$(phase)/porcupine/$(path)\",\n",
    "        readdir(\"./dataset/$(phase)/porcupine/\")\n",
    "    )\n",
    "    vcat(hedgehogs, porcupines)\n",
    "end\n",
    "\n",
    "train_list = make_dataset_list(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-0.0458088 -0.319805 … -0.439678 -0.30268; -0.232493 -0.722689 … -1.82563 -1.68557; -0.741264 -0.932985 … -0.706405 -0.566972]\n",
       "\n",
       "Float32[-0.217056 -0.097183 … -0.508177 -0.491052; -0.442577 -0.60014 … -1.82563 -1.84314; -0.81098 -0.81098 … -0.723834 -0.741264]\n",
       "\n",
       "Float32[0.913177 -0.234181 … -0.542427 -0.491052; 0.677871 -0.617647 … -1.82563 -1.80812; 0.531068 -0.758693 … -0.706405 -0.688976]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.810429 0.604932 … -1.77541 -1.72403; 0.572829 0.432773 … -2.01821 -2.01821; 0.26963 0.47878 … -1.7173 -1.69987]\n",
       "\n",
       "Float32[0.998801 1.66667 … -1.74116 -1.68979; 0.712885 1.69328 … -2.01821 -2.03571; 0.600784 1.82083 … -1.66501 -1.64758]\n",
       "\n",
       "Float32[1.87216 2.19753 … -1.70691 -1.67266; 1.7458 2.35854 … -2.01821 -2.03571; 1.76854 2.43085 … -1.66501 -1.64758], 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "@pydef mutable struct Dataset <: torch.utils.data.Dataset\n",
    "    __init__(self, phase::String=\"phase\") = begin\n",
    "        pybuiltin(:super)(Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.file_list = make_dataset_list(phase)\n",
    "    end\n",
    "    \n",
    "    __len__(self) = length(self.file_list)\n",
    "    \n",
    "    __getitem__(self, index::Int) = begin\n",
    "        # index番目の画像をロード\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = image_transform_vgg16(img; phase=self.phase)\n",
    "        # 画像のラベル名をパスから抜き出す\n",
    "        label = img_path[length(self.phase) + 12 : length(self.phase) + 19]\n",
    "        # ハリネズミ: 0, ヤマアラシ: 1\n",
    "        label = (label == \"hedgehog\" ? 0 : 1)\n",
    "        return img_transformed, label\n",
    "    end\n",
    "end\n",
    "\n",
    "train_dataset = Dataset(\"train\")\n",
    "val_dataset = Dataset(\"val\")\n",
    "\n",
    "# 動作確認\n",
    "index = 1\n",
    "img_transformed, label = train_dataset.__getitem__(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,PyObject} with 2 entries:\n",
       "  \"val\"   => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7f4b5…\n",
       "  \"train\" => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7f4b5…"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ミニバッチサイズ\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"val\" => val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `setindex!(o::PyObject, v, i::Integer)` is deprecated, use `set!(o, i - 1, v)` instead.\n",
      "│   caller = top-level scope at In[30]:3\n",
      "└ @ Core In[30]:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みVGG-16モデルをロード\n",
    "net = models.vgg16(pretrained=true)\n",
    "\n",
    "# VGG-16の最後の全結合出力層の出力ユニットを2個に付け替える\n",
    "## 出力は ハリネズミ=0, ヤマアラシ=1 の2種類分類\n",
    "net.classifier[7] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "----------\n",
      "Any[PyObject Parameter containing:\n",
      "tensor([[ 0.0006,  0.0061, -0.0034,  ...,  0.0021,  0.0064, -0.0136],\n",
      "        [-0.0058, -0.0038,  0.0030,  ..., -0.0017,  0.0088, -0.0056]],\n",
      "       requires_grad=True), PyObject Parameter containing:\n",
      "tensor([-0.0119, -0.0029], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 転移学習で学習させるパラメータを params_to_update に格納\n",
    "params_to_update = []\n",
    "\n",
    "# 学習させるパラメータ名\n",
    "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "# 学習させるパラメータ以外は勾配計算させない\n",
    "for (name, param) in net.named_parameters()\n",
    "    if in(name, update_param_names)\n",
    "        param.required_grad = true\n",
    "        push!(params_to_update, param)\n",
    "        println(name)\n",
    "    else\n",
    "        param.required_grad = false\n",
    "    end\n",
    "end\n",
    "\n",
    "# params_to_updateの中身を確認\n",
    "println(\"----------\")\n",
    "println(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\r",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n"
     ]
    },
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError (ccall(#= /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:11 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), o)) <class 'IndexError'>\nIndexError('Julia exception: BoundsError: attempt to access 80-element Array{String,1} at index [0]\\nStacktrace:\\n [1] getindex(::Array{String,1}, ::Int64) at ./array.jl:729\\n [2] ##__getitem__#381(::PyObject, ::Int64) at ./In[28]:13\\n [3] (::getfield(PyCall, Symbol(\"#f_kw_closure#55\")){getfield(Main, Symbol(\"###__getitem__#381\")),Tuple{PyObject,Int64},Array{Tuple{Symbol,Any},1}})() at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:36\\n [4] _pyjlwrap_call(::Function, ::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}) at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:37\\n [5] pyjlwrap_call(::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}) at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:49\\n [6] _start(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:11\\n [7] iterate(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:91\\n [8] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[35]:27\\n [9] top-level scope at In[35]:54\\n [10] eval at ./boot.jl:328 [inlined]\\n [11] softscope_include_string(::Module, ::String, ::String) at /home/user/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\\n [12] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/user/.julia/packages/IJulia/cwvsj/src/execute_request.jl:67\\n [13] #invokelatest#1 at ./essentials.jl:742 [inlined]\\n [14] invokelatest at ./essentials.jl:741 [inlined]\\n [15] eventloop(::ZMQ.Socket) at /home/user/.julia/packages/IJulia/cwvsj/src/eventloop.jl:8\\n [16] (::getfield(IJulia, Symbol(\"##15#18\")))() at ./task.jl:259')\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 937, in __iter__\n    for obj in iterable:\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in <listcomp>\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"PyCall\", line 1, in <lambda>\n",
     "output_type": "error",
     "traceback": [
      "PyError (ccall(#= /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:11 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), o)) <class 'IndexError'>\nIndexError('Julia exception: BoundsError: attempt to access 80-element Array{String,1} at index [0]\\nStacktrace:\\n [1] getindex(::Array{String,1}, ::Int64) at ./array.jl:729\\n [2] ##__getitem__#381(::PyObject, ::Int64) at ./In[28]:13\\n [3] (::getfield(PyCall, Symbol(\"#f_kw_closure#55\")){getfield(Main, Symbol(\"###__getitem__#381\")),Tuple{PyObject,Int64},Array{Tuple{Symbol,Any},1}})() at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:36\\n [4] _pyjlwrap_call(::Function, ::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}) at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:37\\n [5] pyjlwrap_call(::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}, ::Ptr{PyCall.PyObject_struct}) at /home/user/.julia/packages/PyCall/ttONZ/src/callback.jl:49\\n [6] _start(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:11\\n [7] iterate(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:91\\n [8] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[35]:27\\n [9] top-level scope at In[35]:54\\n [10] eval at ./boot.jl:328 [inlined]\\n [11] softscope_include_string(::Module, ::String, ::String) at /home/user/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\\n [12] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/user/.julia/packages/IJulia/cwvsj/src/execute_request.jl:67\\n [13] #invokelatest#1 at ./essentials.jl:742 [inlined]\\n [14] invokelatest at ./essentials.jl:741 [inlined]\\n [15] eventloop(::ZMQ.Socket) at /home/user/.julia/packages/IJulia/cwvsj/src/eventloop.jl:8\\n [16] (::getfield(IJulia, Symbol(\"##15#18\")))() at ./task.jl:259')\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 937, in __iter__\n    for obj in iterable:\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in <listcomp>\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"PyCall\", line 1, in <lambda>\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /home/user/.julia/packages/PyCall/ttONZ/src/exception.jl:60 [inlined]",
      " [2] _start(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:11",
      " [3] iterate(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:91",
      " [4] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[35]:27",
      " [5] top-level scope at In[35]:54"
     ]
    }
   ],
   "source": [
    "# モデル訓練\n",
    "train_model(net, dataloaders, criterion, optimizer, num_epochs) = begin\n",
    "    tqdm = pyimport(\"tqdm\").tqdm\n",
    "    \n",
    "    # epoch数分ループ\n",
    "    for epoch = 1:num_epochs\n",
    "        println(\"Epoch $(epoch)/$(num_epochs)\")\n",
    "        println(\"----------\")\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in [\"train\", \"val\"]\n",
    "            if phase == \"train\"\n",
    "                net.train() # 訓練モードに\n",
    "            else\n",
    "                net.eval() # 検証モードに\n",
    "            end\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # 未学習時の検証性能を確かめるため、最初の訓練は省略\n",
    "            if epoch == 1 && phase == \"train\"\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for (inputs, labels) in tqdm(dataloaders[phase])\n",
    "                # optimizer初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝搬計算\n",
    "                torch.set_grad_enabled(phase == \"train\")\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outpus, labels) # 損失計算\n",
    "                (max, preds) = torch.max(outputs, 1) # ラベルを予測\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == \"train\"\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                end\n",
    "                # イテレーション結果の計算\n",
    "                epoch_loss += loss.item() * iputs.size(0)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "                torch.set_grad_enabled(false)\n",
    "            end\n",
    "            \n",
    "            # epochごとの損失と正解率を表示\n",
    "            epoch_loss = epoch_loss / length(dataloaders[phase].dataset)\n",
    "            epoch_acc = epoch_corrects^2 / length(dataloaders[phase].dataset)\n",
    "            println(\"$(phase) Loss: $(epoch_loss), Acc: $(epoch_acc)\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
