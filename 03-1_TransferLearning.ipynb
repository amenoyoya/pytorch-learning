{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchを使った転移学習\n",
    "\n",
    "## 転移学習\n",
    "\n",
    "- **転移学習**\n",
    "    - 学習済みのモデルの層の一部を付け替えて、新しいパラメータを学習させるディープラーニング手法の一つ\n",
    "    - 一から学習させる場合に比べて少ない教師データと時間で学習させることができる\n",
    "- 学習済みモデルの使い方\n",
    "    - 基本的に現在学習済みモデルとして公開されているものは、ほぼ全てPythonフレームワークで作られたものである\n",
    "    - DeepLearningモデルを様々なフレームワーク間で交換するためのフォーマットとして**ONNX**(オニキス)形式が提唱されている\n",
    "        - JuliaのネイティブDeepLearningフレームワーク「Flux」用にONNXモデルをインポートするライブラリもある\n",
    "        - 現時点では、まだ開発途中で完全にONNXモデルをロードすることはできない\n",
    "    - Juliaのフレームワーク等が充実するまではPyCallを介してPyTorchなどのフレームワークを使うのが良いかもしれない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./lib/Image.jl\")\n",
    "include(\"./lib/TorchVision.jl\")\n",
    "using .TorchVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <torch._C.Generator object at 0x7f1bd7cd8c30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "# 乱数初期化\n",
    "## Random.seed!([rng=GLOBAL_RNG], seed) -> rng\n",
    "## Random.seed!([rng=GLOBAL_RNG]) -> rng\n",
    "### `!`付きの関数は第一引数の値を破壊的に変更する\n",
    "Random.seed!(1234)\n",
    "\n",
    "# PyTorchの乱数初期化\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#3 (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "# 訓練用、予測用の画像変換関数を作成する関数\n",
    "## () -> ((PyObject, String) -> Array{Float32,3})\n",
    "make_transformer_for_learning() = begin\n",
    "    resize = 224\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    transform = Dict(\n",
    "        \"train\" => make_transformer(\n",
    "            transforms.RandomResizedCrop(resize; scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ),\n",
    "        \"val\" => make_transformer(\n",
    "            transforms.Resize(resize),\n",
    "            transforms.CenterCrop(resize),\n",
    "            transforms.Normalize(mean, std)\n",
    "        )\n",
    "    )\n",
    "    return (image::PyObject; phase::String=\"train\") -> transform[phase](image)\n",
    "end\n",
    "\n",
    "image_transform_vgg16 = make_transformer_for_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585-element Array{String,1}:\n",
       " \"./dataset/train/hedgehog/118523311_32345c36a2.jpg\"    \n",
       " \"./dataset/train/hedgehog/1241612498_7ab4277d10.jpg\"   \n",
       " \"./dataset/train/hedgehog/126009980_9004803c9e.jpg\"    \n",
       " \"./dataset/train/hedgehog/1274493397_88388552d8.jpg\"   \n",
       " \"./dataset/train/hedgehog/127772208_f65a074ed5.jpg\"    \n",
       " \"./dataset/train/hedgehog/1295991716_4ad47dae66.jpg\"   \n",
       " \"./dataset/train/hedgehog/1296287640_19d39d5b1e.jpg\"   \n",
       " \"./dataset/train/hedgehog/1322807353_6eec9596b3.jpg\"   \n",
       " \"./dataset/train/hedgehog/150464690_e33dd1938d.jpg\"    \n",
       " \"./dataset/train/hedgehog/159959475_fb41beb469.jpg\"    \n",
       " \"./dataset/train/hedgehog/163878245_fd30b5169b.jpg\"    \n",
       " \"./dataset/train/hedgehog/17404099_32851ad117.jpg\"     \n",
       " \"./dataset/train/hedgehog/176380875_d2ad991223.jpg\"    \n",
       " ⋮                                                      \n",
       " \"./dataset/train/porcupine/PA210066.JPG\"               \n",
       " \"./dataset/train/porcupine/porcupine_sc108.jpg\"        \n",
       " \"./dataset/train/porcupine/porcupine_sud_america.jpg\"  \n",
       " \"./dataset/train/porcupine/puerco_espin_comun.jpg\"     \n",
       " \"./dataset/train/porcupine/somekinda-porcupine-big.jpg\"\n",
       " \"./dataset/train/porcupine/speaking-porcupine2.jpg\"    \n",
       " \"./dataset/train/porcupine/specCRFS_02Istrice.jpg\"     \n",
       " \"./dataset/train/porcupine/tn-porcoespinho.jpg\"        \n",
       " \"./dataset/train/porcupine/tn_porcupine_jpg.jpg\"       \n",
       " \"./dataset/train/porcupine/yun_3882.jpg\"               \n",
       " \"./dataset/train/porcupine/zoo+070.jpg\"                \n",
       " \"./dataset/train/porcupine/zporcupine.jpg\"             "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシの画像へのファイルパスのリスト作成\n",
    "make_dataset_list(phase::String=\"train\") = begin\n",
    "    hedgehogs = map(\n",
    "        path -> \"./dataset/$(phase)/hedgehog/$(path)\",\n",
    "        readdir(\"./dataset/$(phase)/hedgehog/\")\n",
    "    )\n",
    "    porcupines = map(\n",
    "        path -> \"./dataset/$(phase)/porcupine/$(path)\",\n",
    "        readdir(\"./dataset/$(phase)/porcupine/\")\n",
    "    )\n",
    "    vcat(hedgehogs, porcupines)\n",
    "end\n",
    "\n",
    "train_list = make_dataset_list(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-2.1179 -2.1179 … -2.1179 -2.1179; -2.03571 -2.03571 … -2.03571 -2.03571; -1.80444 -1.80444 … -1.80444 -1.80444]\n",
       "\n",
       "Float32[-2.1179 -2.1179 … -2.1179 -2.1179; -2.03571 -2.03571 … -2.03571 -2.03571; -1.80444 -1.80444 … -1.80444 -1.80444]\n",
       "\n",
       "Float32[-2.1179 -2.1179 … -2.1179 -2.1179; -2.03571 -2.03571 … -2.03571 -2.03571; -1.80444 -1.80444 … -1.80444 -1.80444]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.536433 0.964552 … -0.422553 0.211063; 0.170168 0.590336 … -0.302521 0.345238; 0.0604794 0.531068 … -0.0789542 0.565926]\n",
       "\n",
       "Float32[1.01593 0.930302 … 0.211063 1.80366; 0.677871 0.590336 … 0.345238 1.97339; 0.548497 0.409063 … 0.565926 2.18684]\n",
       "\n",
       "Float32[0.656306 0.570682 … 1.18717 -0.0458088; 0.310224 0.257703 … 1.34314 0.0826331; 0.112767 -0.0789542 … 1.55939 0.304488], 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "@pydef mutable struct Dataset <: torch.utils.data.Dataset\n",
    "    __init__(self, phase::String=\"phase\") = begin\n",
    "        pybuiltin(:super)(Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.file_list = make_dataset_list(phase)\n",
    "    end\n",
    "    \n",
    "    __len__(self) = length(self.file_list)\n",
    "    \n",
    "    __getitem__(self, index::Int) = begin\n",
    "        # index番目の画像をロード\n",
    "        ## Juliaのindexは1〜なので +1 する\n",
    "        img_path = self.file_list[index + 1]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = image_transform_vgg16(img; phase=self.phase)\n",
    "        # 画像のラベル名をパスから抜き出す\n",
    "        label = img_path[length(self.phase) + 12 : length(self.phase) + 19]\n",
    "        # ハリネズミ: 0, ヤマアラシ: 1\n",
    "        label = (label == \"hedgehog\" ? 0 : 1)\n",
    "        return img_transformed, label\n",
    "    end\n",
    "end\n",
    "\n",
    "train_dataset = Dataset(\"train\")\n",
    "val_dataset = Dataset(\"val\")\n",
    "\n",
    "# 動作確認\n",
    "index = 0\n",
    "img_transformed, label = train_dataset.__getitem__(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,PyObject} with 2 entries:\n",
       "  \"val\"   => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7f1b9…\n",
       "  \"train\" => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7f1b9…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ミニバッチサイズ\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"val\" => val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `setindex!(o::PyObject, v, i::Integer)` is deprecated, use `set!(o, i - 1, v)` instead.\n",
      "│   caller = top-level scope at In[7]:3\n",
      "└ @ Core In[7]:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みVGG-16モデルをロード\n",
    "net = models.vgg16(pretrained=true)\n",
    "\n",
    "# VGG-16の最後の全結合出力層の出力ユニットを2個に付け替える\n",
    "## 出力は ハリネズミ=0, ヤマアラシ=1 の2種類分類\n",
    "net.classifier[7] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "----------\n",
      "Any[PyObject Parameter containing:\n",
      "tensor([[-0.0109,  0.0036, -0.0132,  ...,  0.0019,  0.0018, -0.0121],\n",
      "        [-0.0107,  0.0017,  0.0034,  ...,  0.0134,  0.0052, -0.0079]],\n",
      "       requires_grad=True), PyObject Parameter containing:\n",
      "tensor([-0.0060,  0.0011], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 転移学習で学習させるパラメータを params_to_update に格納\n",
    "params_to_update = []\n",
    "\n",
    "# 学習させるパラメータ名\n",
    "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "# 学習させるパラメータ以外は勾配計算させない\n",
    "for (name, param) in net.named_parameters()\n",
    "    if in(name, update_param_names)\n",
    "        param.required_grad = true\n",
    "        push!(params_to_update, param)\n",
    "        println(name)\n",
    "    else\n",
    "        param.required_grad = false\n",
    "    end\n",
    "end\n",
    "\n",
    "# params_to_updateの中身を確認\n",
    "println(\"----------\")\n",
    "println(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\r",
      " 33%|██████████████▋                             | 1/3 [00:00<00:00,  5.73it/s]"
     ]
    },
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError (ccall(#= /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:81 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), s[2])) <class 'RuntimeError'>\nRuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\")\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 937, in __iter__\n    for obj in iterable:\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in <listcomp>\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"PyCall\", line 1, in <lambda>\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 164, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 208, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
     "output_type": "error",
     "traceback": [
      "PyError (ccall(#= /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:81 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), s[2])) <class 'RuntimeError'>\nRuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\")\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 937, in __iter__\n    for obj in iterable:\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in <listcomp>\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"PyCall\", line 1, in <lambda>\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 164, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/user/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 208, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /home/user/.julia/packages/PyCall/ttONZ/src/exception.jl:60 [inlined]",
      " [2] iterate(::PyCall.PyIterator{PyAny,Base.SizeUnknown}, ::Tuple{PyObject,PyObject}) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:81",
      " [3] iterate at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:92 [inlined]",
      " [4] iterate(::PyObject) at /home/user/.julia/packages/PyCall/ttONZ/src/pyiterator.jl:91",
      " [5] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[10]:27",
      " [6] top-level scope at In[10]:54"
     ]
    }
   ],
   "source": [
    "# モデル訓練\n",
    "train_model(net, dataloaders, criterion, optimizer, num_epochs) = begin\n",
    "    tqdm = pyimport(\"tqdm\").tqdm\n",
    "    \n",
    "    # epoch数分ループ\n",
    "    for epoch = 1:num_epochs\n",
    "        println(\"Epoch $(epoch)/$(num_epochs)\")\n",
    "        println(\"----------\")\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in [\"train\", \"val\"]\n",
    "            if phase == \"train\"\n",
    "                net.train() # 訓練モードに\n",
    "            else\n",
    "                net.eval() # 検証モードに\n",
    "            end\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # 未学習時の検証性能を確かめるため、最初の訓練は省略\n",
    "            if epoch == 1 && phase == \"train\"\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            ## tqdmによるプログレスバーは、Julia＋JupyterNotebookではリアルタイム描画されないため、正直意味はない\n",
    "            for (inputs, labels) in tqdm(dataloaders[phase])\n",
    "                # optimizer初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝搬計算\n",
    "                torch.set_grad_enabled(phase == \"train\")\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels) # 損失計算\n",
    "                (max, preds) = torch.max(outputs, 1) # ラベルを予測\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == \"train\"\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                end\n",
    "                # イテレーション結果の計算\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "                torch.set_grad_enabled(false)\n",
    "            end\n",
    "            \n",
    "            # epochごとの損失と正解率を表示\n",
    "            epoch_loss = epoch_loss / length(dataloaders[phase].dataset)\n",
    "            epoch_acc = epoch_corrects^2 / length(dataloaders[phase].dataset)\n",
    "            println(\"$(phase) Loss: $(epoch_loss), Acc: $(epoch_acc)\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\") について\n",
    "これは、グレースケール画像が混ざっているために起こるエラーである\n",
    "\n",
    "本来は、グレースケールの画像を探し出して削除するのが良いのだが、面倒なので、画像読み込み時にRGB画像として読み込むように変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n",
      "val Loss: 0.6778282999992371, Acc: PyObject tensor(23)\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\r",
      " 33%|██████████████▋                             | 1/3 [00:00<00:00,  4.78it/s]\r",
      " 67%|█████████████████████████████▎              | 2/3 [00:04<00:01,  1.35s/it]\r",
      "100%|████████████████████████████████████████████| 3/3 [00:08<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3890837358103858, Acc: PyObject tensor(407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|                                                   | 0/19 [00:00<?, ?it/s]\r",
      "  5%|██▎                                        | 1/19 [00:00<00:03,  4.94it/s]\r",
      " 11%|████▌                                      | 2/19 [00:11<00:59,  3.48s/it]\r",
      " 16%|██████▊                                    | 3/19 [00:22<01:34,  5.91s/it]\r",
      " 21%|█████████                                  | 4/19 [00:33<01:51,  7.45s/it]\r",
      " 26%|███████████▎                               | 5/19 [00:45<01:59,  8.54s/it]\r",
      " 32%|█████████████▌                             | 6/19 [00:56<02:01,  9.37s/it]\r",
      " 37%|███████████████▊                           | 7/19 [01:08<02:01, 10.09s/it]\r",
      " 42%|██████████████████                         | 8/19 [01:19<01:54, 10.40s/it]\r",
      " 47%|████████████████████▎                      | 9/19 [01:30<01:46, 10.64s/it]\r",
      " 53%|██████████████████████                    | 10/19 [01:41<01:37, 10.80s/it]\r",
      " 58%|████████████████████████▎                 | 11/19 [01:53<01:27, 10.98s/it]\r",
      " 63%|██████████████████████████▌               | 12/19 [02:05<01:19, 11.40s/it]\r",
      " 68%|████████████████████████████▋             | 13/19 [02:16<01:08, 11.35s/it]\r",
      " 74%|██████████████████████████████▉           | 14/19 [02:27<00:56, 11.36s/it]\r",
      " 79%|█████████████████████████████████▏        | 15/19 [02:39<00:45, 11.29s/it]\r",
      " 84%|███████████████████████████████████▎      | 16/19 [02:50<00:33, 11.27s/it]\r",
      " 89%|█████████████████████████████████████▌    | 17/19 [03:01<00:22, 11.27s/it]\r",
      " 95%|███████████████████████████████████████▊  | 18/19 [03:13<00:11, 11.38s/it]\r",
      "100%|██████████████████████████████████████████| 19/19 [03:24<00:00, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3321733415126801, Acc: PyObject tensor(61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]\r",
      " 33%|██████████████▋                             | 1/3 [00:00<00:00,  4.17it/s]\r",
      " 67%|█████████████████████████████▎              | 2/3 [00:03<00:01,  1.25s/it]\r",
      "100%|████████████████████████████████████████████| 3/3 [00:07<00:00,  1.96s/it]"
     ]
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "## ※ 画像をRGB画像として読み込む\n",
    "@pydef mutable struct Dataset <: torch.utils.data.Dataset\n",
    "    __init__(self, phase::String=\"phase\") = begin\n",
    "        pybuiltin(:super)(Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.file_list = make_dataset_list(phase)\n",
    "    end\n",
    "    \n",
    "    __len__(self) = length(self.file_list)\n",
    "    \n",
    "    __getitem__(self, index::Int) = begin\n",
    "        # index番目の画像をロード\n",
    "        ## Juliaのindexは1〜なので +1 する\n",
    "        img_path = self.file_list[index + 1]\n",
    "        img = Image.open(img_path).convert(\"RGB\") # ←追加\n",
    "        img_transformed = image_transform_vgg16(img; phase=self.phase)\n",
    "        # 画像のラベル名をパスから抜き出す\n",
    "        label = img_path[length(self.phase) + 12 : length(self.phase) + 19]\n",
    "        # ハリネズミ: 0, ヤマアラシ: 1\n",
    "        label = (label == \"hedgehog\" ? 0 : 1)\n",
    "        return img_transformed, label\n",
    "    end\n",
    "end\n",
    "\n",
    "train_dataset = Dataset(\"train\")\n",
    "val_dataset = Dataset(\"val\")\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"val\" => val_dataloader\n",
    ")\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float32,2}:\n",
       " -1.14885  1.74748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 転移学習したモデルで改めてハリネズミ画像を認識させる\n",
    "\n",
    "net.eval() # 推論モードに設定\n",
    "\n",
    "# 画像読み込み\n",
    "image_file_path = \"./data/gahag-0059907781-1.jpg\"\n",
    "img = Image.open(image_file_path)\n",
    "\n",
    "# 画像をVGG16に読み込ませられるように処理する\n",
    "transform = make_transformer_for_vgg16()\n",
    "img_transformed = transform(img)\n",
    "\n",
    "# 転移学習したVGG-16モデルで予測実行\n",
    "pred = predict(net, [img_transformed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベルは `[ハリネズミ, ヤマアラシ]` と定義したため、上記の予測は `ヤマアラシ` という結果を表している\n",
    "\n",
    "したがって、今回の転移学習は失敗したということができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float32,2}:\n",
       " -1.42318  1.77903"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ヤマアラシの画像でも予測してみる\n",
    "img2 = Image.open(\"./data/publicdomainq-0025120muq.jpg\")\n",
    "img2_transformed = transform(img2)\n",
    "pred = predict(net, [img2_transformed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 転移学習したモデルのパラメータを保存する\n",
    "torch.save(net.state_dict(), \"./vgg16_weight.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果と考察\n",
    "\n",
    "今回は、上手く転移学習させることができず、ハリネズミとヤマアラシを識別するモデルを作成することはできなかった\n",
    "\n",
    "この原因としては以下のようなものが考えられる\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数に差がありすぎた\n",
    "    - 以下のように、ヤマアラシの画像はハリネズミの画像の5倍近くあり、学習には不向きだった\n",
    "        - 訓練用画像数:\n",
    "            - ハリネズミ:  98枚\n",
    "            - ヤマアラシ: 487枚\n",
    "        - 検証用画像数:\n",
    "            - ハリネズミ: 40枚\n",
    "            - ヤマアラシ: 40枚\n",
    "2. 教師データそのものが誤っている可能性があった\n",
    "    - 人間が手動で分類しており、教師データそのものの妥当性が割と怪しかった\n",
    "3. 教師データ量が足りていなかった\n",
    "4. そもそもVGG-16モデル自体古いモデルであり、精度がそれほど高くない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
