{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 転移学習\n",
    "\n",
    "### 前回の問題点\n",
    "前回のハリネズミ・ヤマアラシ識別モデルは、上手く転移学習することができず、ハリネズミをヤマアラシとして認識してしまった\n",
    "\n",
    "その問題点として以下のようなものがあった\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数に差がありすぎた\n",
    "    - 以下のように、ヤマアラシの画像はハリネズミの画像の5倍近くあり、学習には不向きだった\n",
    "        - 訓練用画像数:\n",
    "            - ハリネズミ:  98枚\n",
    "            - ヤマアラシ: 487枚\n",
    "        - 検証用画像数:\n",
    "            - ハリネズミ: 40枚\n",
    "            - ヤマアラシ: 40枚\n",
    "2. 教師データそのものが誤っている可能性があった\n",
    "    - 人間が手動で分類しており、教師データそのものの妥当性が割と怪しかった\n",
    "3. 教師データ量が足りていなかった\n",
    "4. そもそもVGG-16モデル自体古いモデルであり、精度がそれほど高くない\n",
    "\n",
    "ここでは、データを増やしたり、モデルそのものを変更するという面倒なことはせず、簡単にできそうな 1, 2 の対策を行い、学習精度が向上するか実験してみる\n",
    "\n",
    "### 教師データの選別\n",
    "以下の対応を行い、教師データを選別した\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数が同一になるように一部データを削除\n",
    "2. ハリネズミなのかヤマアラシなのか怪しい画像は削除\n",
    "\n",
    "結果、教師データは以下の数となった\n",
    "\n",
    "- 訓練用画像数\n",
    "    - ハリネズミ: 90枚\n",
    "    - ヤマアラシ: 90枚\n",
    "- 検証用画像数\n",
    "    - ハリネズミ: 40枚\n",
    "    - ヤマアラシ: 40枚\n",
    "\n",
    "このデータセットを使い、もう一度転移学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Image.\n",
      "WARNING: replacing module TorchVision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <torch._C.Generator object at 0x7fc7b7a3fc30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyCallを使う\n",
    "using PyCall\n",
    "\n",
    "# 自作ライブラリ読み込み\n",
    "include(\"./lib/Image.jl\")\n",
    "include(\"./lib/TorchVision.jl\")\n",
    "using .TorchVision # TorchVisionモジュールのexport変数をそのまま使えるようにする\n",
    "\n",
    "# 乱数初期化\n",
    "seed_random!(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_label (generic function with 2 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 画像をVGG-16入力用に変換する関数を生成\n",
    "## (img::PyObject, phase::Phase) :: Array{Float32,3}\n",
    "transform_image = make_transformer_for_vgg16_training()\n",
    "\n",
    "# ハリネズミとヤマアラシの画像へのファイルパスのリスト作成\n",
    "make_dataset_list(phase::Phase)::Array{String,1} = begin\n",
    "    phasestr = typestr(phase)\n",
    "    hedgehogs = map(\n",
    "        path -> \"./dataset/$(phasestr)/hedgehog/$(path)\",\n",
    "        readdir(\"./dataset/$(phasestr)/hedgehog/\")\n",
    "    )\n",
    "    porcupines = map(\n",
    "        path -> \"./dataset/$(phasestr)/porcupine/$(path)\",\n",
    "        readdir(\"./dataset/$(phasestr)/porcupine/\")\n",
    "    )\n",
    "    vcat(hedgehogs, porcupines)\n",
    "end\n",
    "\n",
    "# 画像のラベルをパスから判定する\n",
    "## ハリネズミ: 0, ヤマアラシ: 1\n",
    "make_label(img_path::String, phase::Phase)::Int = begin\n",
    "    label = match(r\"/([^/]+)/[^/]+$\", img_path).captures[1]\n",
    "    label = (label == \"hedgehog\" ? 0 : 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180-element Array{String,1}:\n",
       " \"./dataset/train/hedgehog/118523311_32345c36a2.jpg\"    \n",
       " \"./dataset/train/hedgehog/1241612498_7ab4277d10.jpg\"   \n",
       " \"./dataset/train/hedgehog/126009980_9004803c9e.jpg\"    \n",
       " \"./dataset/train/hedgehog/127772208_f65a074ed5.jpg\"    \n",
       " \"./dataset/train/hedgehog/150464690_e33dd1938d.jpg\"    \n",
       " \"./dataset/train/hedgehog/159959475_fb41beb469.jpg\"    \n",
       " \"./dataset/train/hedgehog/163878245_fd30b5169b.jpg\"    \n",
       " \"./dataset/train/hedgehog/17404099_32851ad117.jpg\"     \n",
       " \"./dataset/train/hedgehog/176380875_d2ad991223.jpg\"    \n",
       " \"./dataset/train/hedgehog/182814624_da265f061b.jpg\"    \n",
       " \"./dataset/train/hedgehog/190161565_8be2a2f3bf.jpg\"    \n",
       " \"./dataset/train/hedgehog/193309712_3cb1b35e4e.jpg\"    \n",
       " \"./dataset/train/hedgehog/196249860_b546d15d1c.jpg\"    \n",
       " ⋮                                                      \n",
       " \"./dataset/train/porcupine/806165008_22ba40fd7b.jpg\"   \n",
       " \"./dataset/train/porcupine/porcupine_sc108.jpg\"        \n",
       " \"./dataset/train/porcupine/porcupine_sud_america.jpg\"  \n",
       " \"./dataset/train/porcupine/puerco_espin_comun.jpg\"     \n",
       " \"./dataset/train/porcupine/somekinda-porcupine-big.jpg\"\n",
       " \"./dataset/train/porcupine/speaking-porcupine2.jpg\"    \n",
       " \"./dataset/train/porcupine/specCRFS_02Istrice.jpg\"     \n",
       " \"./dataset/train/porcupine/tn-porcoespinho.jpg\"        \n",
       " \"./dataset/train/porcupine/tn_porcupine_jpg.jpg\"       \n",
       " \"./dataset/train/porcupine/yun_3882.jpg\"               \n",
       " \"./dataset/train/porcupine/zoo+070.jpg\"                \n",
       " \"./dataset/train/porcupine/zporcupine.jpg\"             "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 動作確認\n",
    "train_dataset_list = make_dataset_list(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_label(train_dataset_list[1], Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.0 0.0 … 0.490034 0.522377; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.228195 0.27649]\n",
       "\n",
       "Float32[0.0 0.0 … 0.473863 0.514291; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.208877 0.266831]\n",
       "\n",
       "Float32[0.0 0.0 … 0.457692 0.473863; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.189559 0.199218], 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "macro image_dataset(TypeName, make_dataset_list_function, image_transform_function, labeling_function)\n",
    "    esc(quote\n",
    "        @pydef mutable struct $TypeName <: TorchVision.torch.utils.data.Dataset\n",
    "            __init__(self, phase::TorchVision.Phase) = begin\n",
    "                pybuiltin(:super)($TypeName, self).__init__()\n",
    "                self.phase = phase\n",
    "                self.file_list = $make_dataset_list_function(phase)\n",
    "            end\n",
    "\n",
    "            __len__(self) = length(self.file_list)\n",
    "\n",
    "            __getitem__(self, index::Int) = begin\n",
    "                # index番目の画像をロード\n",
    "                ## Juliaのindexは1〜なので +1 する\n",
    "                img_path = self.file_list[index + 1]\n",
    "                img = Image.open(img_path).convert(\"RGB\") # グレースケール画像は強制的にRGB画像に変換\n",
    "                img_transformed = $image_transform_function(img, self.phase)\n",
    "                # ラベリング\n",
    "                label = $labeling_function(img_path, self.phase)\n",
    "                return img_transformed, label\n",
    "            end\n",
    "        end\n",
    "    end)\n",
    "end\n",
    "\n",
    "@image_dataset Dataset make_dataset_list transform_image make_label\n",
    "\n",
    "train_dataset = Dataset(Train)\n",
    "val_dataset = Dataset(Valid)\n",
    "\n",
    "# 動作確認\n",
    "index = 0\n",
    "img_transformed, label = train_dataset.__getitem__(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ミニバッチサイズ\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"val\" => val_dataloader\n",
    ")\n",
    "\n",
    "# 学習済みVGG-16モデルをロード\n",
    "net = models.vgg16(pretrained=true)\n",
    "\n",
    "# VGG-16の最後の全結合出力層の出力ユニットを2個に付け替える\n",
    "## 出力は ハリネズミ=0, ヤマアラシ=1 の2種類分類\n",
    "set!(net.classifier, 6, torch.nn.Linear(in_features=4096, out_features=2))\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "----------\n",
      "Any[PyObject Parameter containing:\n",
      "tensor([[-0.0005,  0.0104, -0.0030,  ...,  0.0135, -0.0028,  0.0054],\n",
      "        [-0.0095, -0.0099,  0.0092,  ..., -0.0098,  0.0035,  0.0153]],\n",
      "       requires_grad=True), PyObject Parameter containing:\n",
      "tensor([0.0109, 0.0109], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 転移学習で学習させるパラメータを params_to_update に格納\n",
    "params_to_update = []\n",
    "\n",
    "# 学習させるパラメータ名\n",
    "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "# 学習させるパラメータ以外は勾配計算させない\n",
    "for (name, param) in net.named_parameters()\n",
    "    if in(name, update_param_names)\n",
    "        param.required_grad = true\n",
    "        push!(params_to_update, param)\n",
    "        println(name)\n",
    "    else\n",
    "        param.required_grad = false\n",
    "    end\n",
    "end\n",
    "\n",
    "# params_to_updateの中身を確認\n",
    "println(\"----------\")\n",
    "println(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/user/.julia/compiled/v1.1/ProgressMeter/3V8n6.ji for ProgressMeter [92933f4c-e287-5a05-a399-4b506db050ca]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Epoch 1/2\n",
      "└ @ Main In[37]:10\n",
      "┌ Info: ----------\n",
      "└ @ Main In[37]:11\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:20\u001b[39m\n",
      "┌ Info: train Loss: 0.7628640294075012, Acc: PyObject tensor(32)\n",
      "└ @ Main In[37]:57\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"valid\" not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key \"valid\" not found",
      "",
      "Stacktrace:",
      " [1] getindex at ./dict.jl:478 [inlined]",
      " [2] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[37]:32",
      " [3] top-level scope at In[37]:61"
     ]
    }
   ],
   "source": [
    "using ProgressMeter\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# モデル訓練\n",
    "train_model(net, dataloaders, criterion, optimizer, num_epochs) = begin\n",
    "    # epoch数分ループ\n",
    "    for epoch = 1:num_epochs\n",
    "        @info \"Epoch $(epoch)/$(num_epochs)\"\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase = [Train, Valid]\n",
    "            if phase == Type{Train}\n",
    "                net.train() # 訓練モードに\n",
    "            else\n",
    "                net.eval() # 検証モードに\n",
    "            end\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # 未学習時の検証性能を確かめるため、最初の訓練は省略\n",
    "            if epoch == 1 && phase == Type{Train}\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            ## tqdmによるプログレスバーは、Julia＋JupyterNotebookではリアルタイム描画されないため、正直意味はない\n",
    "            phasestr = typestr(phase)\n",
    "            progress = Progress(dataloaders[phasestr].__len__())\n",
    "            for (inputs, labels) = dataloaders[phasestr].__iter__()\n",
    "                # optimizer初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝搬計算\n",
    "                torch.set_grad_enabled(phase == Type{Train})\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels) # 損失計算\n",
    "                (max, preds) = torch.max(outputs, 1) # ラベルを予測\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == Type{Train}\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                end\n",
    "                # イテレーション結果の計算\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "                torch.set_grad_enabled(false)\n",
    "                next!(progress)\n",
    "            end\n",
    "            \n",
    "            # epochごとの損失と正解率を表示\n",
    "            epoch_loss = epoch_loss / length(dataloaders[phasestr].dataset)\n",
    "            epoch_acc = epoch_corrects^2 / length(dataloaders[phasestr].dataset)\n",
    "            @info \"$(phasestr) Loss: $(epoch_loss), Acc: $(epoch_acc)\"\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
