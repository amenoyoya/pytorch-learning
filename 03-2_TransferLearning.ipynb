{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 転移学習\n",
    "\n",
    "### 前回の問題点\n",
    "前回のハリネズミ・ヤマアラシ識別モデルは、上手く転移学習することができず、ハリネズミをヤマアラシとして認識してしまった\n",
    "\n",
    "その問題点として以下のようなものがあった\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数に差がありすぎた\n",
    "    - 以下のように、ヤマアラシの画像はハリネズミの画像の5倍近くあり、学習には不向きだった\n",
    "        - 訓練用画像数:\n",
    "            - ハリネズミ:  98枚\n",
    "            - ヤマアラシ: 487枚\n",
    "        - 検証用画像数:\n",
    "            - ハリネズミ: 40枚\n",
    "            - ヤマアラシ: 40枚\n",
    "2. 教師データそのものが誤っている可能性があった\n",
    "    - 人間が手動で分類しており、教師データそのものの妥当性が割と怪しかった\n",
    "3. 教師データ量が足りていなかった\n",
    "4. そもそもVGG-16モデル自体古いモデルであり、精度がそれほど高くない\n",
    "\n",
    "ここでは、データを増やしたり、モデルそのものを変更するという面倒なことはせず、簡単にできそうな 1, 2 の対策を行い、学習精度が向上するか実験してみる\n",
    "\n",
    "### 教師データの選別\n",
    "以下の対応を行い、教師データを選別した\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数が同一になるように一部データを削除\n",
    "2. ハリネズミなのかヤマアラシなのか怪しい画像は削除\n",
    "\n",
    "結果、教師データは以下の数となった\n",
    "\n",
    "- 訓練用画像数\n",
    "    - ハリネズミ: 90枚\n",
    "    - ヤマアラシ: 90枚\n",
    "- 検証用画像数\n",
    "    - ハリネズミ: 40枚\n",
    "    - ヤマアラシ: 40枚\n",
    "\n",
    "このデータセットを使い、もう一度転移学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyCallを使う\n",
    "using PyCall\n",
    "\n",
    "# 自作ライブラリ読み込み\n",
    "include(\"./lib/Image.jl\")\n",
    "include(\"./lib/TorchVision.jl\")\n",
    "using .TorchVision # TorchVisionモジュールのexport変数をそのまま使えるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_label (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 画像をVGG-16入力用に変換する関数を生成\n",
    "## (img::PyObject, phase::Phase) :: Array{Float32,3}\n",
    "transform_image = make_transformer_for_vgg16_training()\n",
    "\n",
    "# ハリネズミとヤマアラシの画像へのファイルパスのリスト作成\n",
    "make_dataset_list(phase::Phase)::Array{String,1} = begin\n",
    "    phasestr = typestr(phase)\n",
    "    hedgehogs = map(\n",
    "        path -> \"./dataset/$(phasestr)/hedgehog/$(path)\",\n",
    "        readdir(\"./dataset/$(phasestr)/hedgehog/\")\n",
    "    )\n",
    "    porcupines = map(\n",
    "        path -> \"./dataset/$(phasestr)/porcupine/$(path)\",\n",
    "        readdir(\"./dataset/$(phasestr)/porcupine/\")\n",
    "    )\n",
    "    vcat(hedgehogs, porcupines)\n",
    "end\n",
    "\n",
    "# 画像のラベルをパスから判定する\n",
    "## ハリネズミ: 0, ヤマアラシ: 1\n",
    "make_label(img_path::String, phase::Phase)::Int = begin\n",
    "    label = match(r\"/([^/]+)/[^/]+$\", img_path).captures[1]\n",
    "    label = (label == \"hedgehog\" ? 0 : 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乱数初期化\n",
    "seed_random!(1234)\n",
    "\n",
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "@TorchVision.image_dataset Dataset make_dataset_list transform_image make_label\n",
    "\n",
    "# データローダー作成\n",
    "dataloader = DataLoader(Dataset; batch_size=32, shuffle=true)\n",
    "\n",
    "# 学習済みVGG-16モデルをロード\n",
    "net = models.vgg16(pretrained=true)\n",
    "\n",
    "# VGG-16の最後の全結合出力層の出力ユニットを2個に付け替える\n",
    "## 出力は ハリネズミ=0, ヤマアラシ=1 の2種類分類\n",
    "set!(net.classifier, 6, torch.nn.Linear(in_features=4096, out_features=2))\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 学習させるパラメータ名\n",
    "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "# 学習させるパラメータを設定\n",
    "params_to_update = set_params_to_update!(net, update_param_names)\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1/2\n",
      "└ @ Main In[5]:5\n",
      "┌ Info: Train mode\n",
      "└ @ Main In[5]:18\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:05\u001b[39m\n",
      "┌ Info: Train Loss: 0.6989214499791463, Acc: 57.77777777777777\n",
      "└ @ Main In[5]:29\n",
      "┌ Info: Valid mode\n",
      "└ @ Main In[5]:21\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "┌ Info: Valid Loss: 0.521346914768219, Acc: 72.5\n",
      "└ @ Main In[5]:29\n",
      "┌ Info: Epoch 2/2\n",
      "└ @ Main In[5]:5\n",
      "┌ Info: Train mode\n",
      "└ @ Main In[5]:18\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:04\u001b[39m\n",
      "┌ Info: Train Loss: 0.4562105589442783, Acc: 78.88888888888889\n",
      "└ @ Main In[5]:29\n",
      "┌ Info: Valid mode\n",
      "└ @ Main In[5]:21\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "┌ Info: Valid Loss: 0.4158574640750885, Acc: 78.75\n",
      "└ @ Main In[5]:29\n"
     ]
    }
   ],
   "source": [
    "# モデル訓練\n",
    "train_model!(net, dataloaders, optimizer, criterion, num_epochs) = begin\n",
    "    # epoch数分ループ\n",
    "    for epoch = 1:num_epochs\n",
    "        @info \"Epoch $(epoch)/$(num_epochs)\"\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in [Train, Valid]\n",
    "            # 未学習時の検証性能を確かめるため、最初の訓練は省略\n",
    "            \"\"\"\n",
    "            if epoch == 1 && phase === Train\n",
    "                continue\n",
    "            end\n",
    "            \"\"\"\n",
    "            \n",
    "            if phase === Train\n",
    "                net.train() # 訓練モードに\n",
    "                @info \"Train mode\"\n",
    "            else\n",
    "                net.eval() # 検証モードに\n",
    "                @info \"Valid mode\"\n",
    "            end\n",
    "            \n",
    "            epoch_loss, epoch_corrects = train!(net, phase, dataloader, optimizer, criterion)\n",
    "            \n",
    "            # epochごとの損失と正解率を表示\n",
    "            epoch_loss = epoch_loss / dataloaders[phase].dataset.__len__()\n",
    "            epoch_acc = epoch_corrects / dataloaders[phase].dataset.__len__() * 100\n",
    "            @info \"$(phase) Loss: $(epoch_loss), Acc: $(epoch_acc)\"\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model!(net, dataloader, optimizer, criterion, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject tensor([[ 1.1600, -0.2584],\n",
       "        [ 0.0044,  0.9110]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 転移学習結果の確認\n",
    "net.eval() # 推論モードに設定\n",
    "inputs = [\n",
    "    transform_image(Image.open(\"./data/gahag-0059907781-1.jpg\"), Valid), # ハリネズミ画像を検証用に変換\n",
    "    transform_image(Image.open(\"./data/publicdomainq-0025120muq.jpg\"), Valid), # ヤマアラシ画像を検証用に変換\n",
    "]\n",
    "pred = net(torch.Tensor(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベルは `[ハリネズミ, ヤマアラシ]` と定義したため、上記の予測は `1つ目＝ハリネズミ、2つ目＝ヤマアラシ` という結果を表している\n",
    "\n",
    "したがって、今回の転移学習は成功である\n",
    "\n",
    "このように、教師データを単純に増やすだけでなく、逆に減らす（良質なデータを選別する）ことでもディープラーニングの精度を向上させることができると言える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
