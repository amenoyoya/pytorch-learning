{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchを使った転移学習\n",
    "\n",
    "## 転移学習\n",
    "\n",
    "- **転移学習**\n",
    "    - 学習済みのモデルの層の一部を付け替えて、新しいパラメータを学習させるディープラーニング手法の一つ\n",
    "    - 一から学習させる場合に比べて少ない教師データと時間で学習させることができる\n",
    "- 学習済みモデルの使い方\n",
    "    - 基本的に現在学習済みモデルとして公開されているものは、ほぼ全てPythonフレームワークで作られたものである\n",
    "    - DeepLearningモデルを様々なフレームワーク間で交換するためのフォーマットとして**ONNX**(オニキス)形式が提唱されている\n",
    "        - JuliaのネイティブDeepLearningフレームワーク「Flux」用にONNXモデルをインポートするライブラリもある\n",
    "        - 現時点では、まだ開発途中で完全にONNXモデルをロードすることはできない\n",
    "    - Juliaのフレームワーク等が充実するまではPyCallを介してPyTorchなどのフレームワークを使うのが良いかもしれない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./lib/Image.jl\")\n",
    "include(\"./lib/TorchVision.jl\")\n",
    "using .TorchVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <torch._C.Generator object at 0x7fdc1cfec710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "# 乱数初期化\n",
    "## Random.seed!([rng=GLOBAL_RNG], seed) -> rng\n",
    "## Random.seed!([rng=GLOBAL_RNG]) -> rng\n",
    "### `!`付きの関数は第一引数の値を破壊的に変更する\n",
    "Random.seed!(1234)\n",
    "\n",
    "# PyTorchの乱数初期化\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#21 (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "# 訓練用、予測用の画像変換関数を作成する関数\n",
    "## () -> ((PyObject, String) -> Array{Float32,3})\n",
    "make_transformer_for_learning() = begin\n",
    "    resize = 224\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    transform = Dict(\n",
    "        \"train\" => make_transformer(\n",
    "            transforms.RandomResizedCrop(resize; scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ),\n",
    "        \"valid\" => make_transformer(\n",
    "            transforms.Resize(resize),\n",
    "            transforms.CenterCrop(resize),\n",
    "            transforms.Normalize(mean, std)\n",
    "        )\n",
    "    )\n",
    "    return (image::PyObject; phase::String=\"train\") -> transform[phase](image)\n",
    "end\n",
    "\n",
    "image_transform_vgg16 = make_transformer_for_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585-element Array{String,1}:\n",
       " \"./dataset/train.noise/hedgehog/118523311_32345c36a2.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/1241612498_7ab4277d10.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/126009980_9004803c9e.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/127772208_f65a074ed5.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/1436386422_3be5e6a0ac.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/150464690_e33dd1938d.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/159959475_fb41beb469.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/163878245_fd30b5169b.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/17404099_32851ad117.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/176380875_d2ad991223.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/1791805273_8b51c7af1e.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/182814624_da265f061b.jpg\"\n",
       " \"./dataset/train.noise/hedgehog/190161565_8be2a2f3bf.jpg\"\n",
       " ⋮\n",
       " \"./dataset/train.noise/porcupine/PA210066.JPG\"\n",
       " \"./dataset/train.noise/porcupine/porcupine_sc108.jpg\"\n",
       " \"./dataset/train.noise/porcupine/porcupine_sud_america.jpg\"\n",
       " \"./dataset/train.noise/porcupine/puerco_espin_comun.jpg\"\n",
       " \"./dataset/train.noise/porcupine/somekinda-porcupine-big.jpg\"\n",
       " \"./dataset/train.noise/porcupine/speaking-porcupine2.jpg\"\n",
       " \"./dataset/train.noise/porcupine/specCRFS_02Istrice.jpg\"\n",
       " \"./dataset/train.noise/porcupine/tn-porcoespinho.jpg\"\n",
       " \"./dataset/train.noise/porcupine/tn_porcupine_jpg.jpg\"\n",
       " \"./dataset/train.noise/porcupine/yun_3882.jpg\"\n",
       " \"./dataset/train.noise/porcupine/zoo+070.jpg\"\n",
       " \"./dataset/train.noise/porcupine/zporcupine.jpg\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシの画像へのファイルパスのリスト作成\n",
    "make_dataset_list(dir::AbstractString) = begin\n",
    "    hedgehogs = map(\n",
    "        path -> \"./dataset/$(dir)/hedgehog/$(path)\",\n",
    "        readdir(\"./dataset/$(dir)/hedgehog/\")\n",
    "    )\n",
    "    porcupines = map(\n",
    "        path -> \"./dataset/$(dir)/porcupine/$(path)\",\n",
    "        readdir(\"./dataset/$(dir)/porcupine/\")\n",
    "    )\n",
    "    vcat(hedgehogs, porcupines)\n",
    "end\n",
    "\n",
    "train_list = make_dataset_list(\"train.noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[-2.117904 -2.1007793 … -2.1007793 -2.117904; -2.0357141 -2.0182073 … -2.0182073 -2.0357141; -1.8044444 -1.7870152 … -1.7870152 -1.8044444]\n",
       "\n",
       "Float32[-2.117904 -2.1007793 … -2.1007793 -2.117904; -2.0357141 -2.0182073 … -2.0182073 -2.0357141; -1.8044444 -1.7870152 … -1.7870152 -1.8044444]\n",
       "\n",
       "Float32[-2.117904 -2.1007793 … -2.1007793 -2.117904; -2.0357141 -2.0182073 … -2.0182073 -2.0357141; -1.8044444 -1.7870152 … -1.7870152 -1.8044444]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[-2.1007793 -2.1007793 … 1.5639181 1.1871736; -2.0182073 -2.0182073 … 1.7282913 1.3431373; -1.7870152 -1.7870152 … 1.9428324 1.5593902]\n",
       "\n",
       "Float32[-2.1007793 -2.1007793 … 0.79330426 0.33093593; -2.0182073 -2.0182073 … 0.94047624 0.4677872; -1.7870152 -1.7870152 … 1.1585187 0.68793046]\n",
       "\n",
       "Float32[-2.1007793 -2.1007793 … -1.8267832 -1.9980307; -2.0182073 -2.0182073 … -1.7380952 -1.9131652; -1.7870152 -1.7870152 … -1.5081482 -1.68244], 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "@pydef mutable struct Dataset <: torch.utils.data.Dataset\n",
    "    __init__(self, dir::AbstractString, phase::AbstractString=\"phase\") = begin\n",
    "        pybuiltin(:super)(Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.dir = dir\n",
    "        self.file_list = make_dataset_list(dir)\n",
    "    end\n",
    "    \n",
    "    __len__(self) = length(self.file_list)\n",
    "    \n",
    "    __getitem__(self, index::Int) = begin\n",
    "        # index番目の画像をロード\n",
    "        ## Juliaのindexは1〜なので +1 する\n",
    "        img_path = self.file_list[index + 1]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = image_transform_vgg16(img; phase=self.phase)\n",
    "        # 画像のラベル名をパスから抜き出す\n",
    "        label = img_path[length(self.dir) + 12 : length(self.dir) + 19]\n",
    "        # ハリネズミ: 0, ヤマアラシ: 1\n",
    "        label = (label == \"hedgehog\" ? 0 : 1)\n",
    "        return img_transformed, label\n",
    "    end\n",
    "end\n",
    "\n",
    "train_dataset = Dataset(\"train.noise\", \"train\")\n",
    "val_dataset = Dataset(\"valid\", \"valid\")\n",
    "\n",
    "# 動作確認\n",
    "index = 0\n",
    "img_transformed, label = train_dataset.__getitem__(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,PyObject} with 2 entries:\n",
       "  \"valid\" => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7fdb0…\n",
       "  \"train\" => PyObject <torch.utils.data.dataloader.DataLoader object at 0x7fdb0…"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ミニバッチサイズ\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"valid\" => val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みVGG-16モデルをロード\n",
    "net = models.vgg16(pretrained=true)\n",
    "\n",
    "# VGG-16の最後の全結合出力層の出力ユニットを2個に付け替える\n",
    "## 出力は ハリネズミ=0, ヤマアラシ=1 の2種類分類\n",
    "net.classifier[7] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "----------\n",
      "Any[PyObject Parameter containing:\n",
      "tensor([[ 0.0067,  0.0039, -0.0004,  ..., -0.0064, -0.0119, -0.0136],\n",
      "        [ 0.0090,  0.0015, -0.0058,  ..., -0.0011, -0.0099, -0.0114]],\n",
      "       requires_grad=True), PyObject Parameter containing:\n",
      "tensor([-0.0122,  0.0090], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 転移学習で学習させるパラメータを params_to_update に格納\n",
    "params_to_update = []\n",
    "\n",
    "# 学習させるパラメータ名\n",
    "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "# 学習させるパラメータ以外は勾配計算させない\n",
    "for (name, param) in net.named_parameters()\n",
    "    if in(name, update_param_names)\n",
    "        param.required_grad = true\n",
    "        push!(params_to_update, param)\n",
    "        println(name)\n",
    "    else\n",
    "        param.required_grad = false\n",
    "    end\n",
    "end\n",
    "\n",
    "# params_to_updateの中身を確認\n",
    "println(\"----------\")\n",
    "println(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "PyError (ccall(#= /home/user/.julia/packages/PyCall/tqyST/src/pyiterator.jl:10 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), o)) <class 'RuntimeError'>\nRuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\")\n  File \"/opt/conda/lib/python3.7/site-packages/tqdm/std.py\", line 1167, in __iter__\n    for obj in iterable:\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"PyCall\", line 1, in <lambda>\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 67, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 284, in normalize\n    tensor.sub_(mean).div_(std)\n",
     "output_type": "error",
     "traceback": [
      "PyError (ccall(#= /home/user/.julia/packages/PyCall/tqyST/src/pyiterator.jl:10 =# @pysym(:PyIter_Next), PyPtr, (PyPtr,), o)) <class 'RuntimeError'>\nRuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\")\n  File \"/opt/conda/lib/python3.7/site-packages/tqdm/std.py\", line 1167, in __iter__\n    for obj in iterable:\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"PyCall\", line 1, in <lambda>\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 67, in __call__\n    img = t(img)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 284, in normalize\n    tensor.sub_(mean).div_(std)\n",
      "",
      "Stacktrace:",
      " [1] pyerr_check at /home/user/.julia/packages/PyCall/tqyST/src/exception.jl:62 [inlined]",
      " [2] #40 at /home/user/.julia/packages/PyCall/tqyST/src/pyiterator.jl:10 [inlined]",
      " [3] disable_sigint at ./c.jl:446 [inlined]",
      " [4] _start at /home/user/.julia/packages/PyCall/tqyST/src/pyiterator.jl:8 [inlined]",
      " [5] iterate(::PyObject) at /home/user/.julia/packages/PyCall/tqyST/src/pyiterator.jl:85",
      " [6] train_model(::PyObject, ::Dict{String,PyObject}, ::PyObject, ::PyObject, ::Int64) at ./In[19]:28",
      " [7] top-level scope at In[19]:59",
      " [8] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "# モデル訓練\n",
    "train_model(net, dataloaders, criterion, optimizer, num_epochs) = begin\n",
    "    tqdm = pyimport(\"tqdm\").tqdm\n",
    "    \n",
    "    # epoch数分ループ\n",
    "    for epoch = 1:num_epochs\n",
    "        println(\"Epoch $(epoch)/$(num_epochs)\")\n",
    "        println(\"----------\")\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in [\"train\", \"valid\"]\n",
    "            if phase == \"train\"\n",
    "                net.train() # 訓練モードに\n",
    "            else\n",
    "                net.eval() # 検証モードに\n",
    "            end\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # 未学習時の検証性能を確かめるため、最初の訓練は省略\n",
    "            if epoch == 1 && phase == \"train\"\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            ## tqdmによるプログレスバーは、Julia＋JupyterNotebookではリアルタイム描画されないため、正直意味はない\n",
    "            for (inputs, labels) in tqdm(dataloaders[phase])\n",
    "                # optimizer初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝搬計算\n",
    "                torch.set_grad_enabled(phase == \"train\")\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels) # 損失計算\n",
    "                (max, preds) = torch.max(outputs, 1) # ラベルを予測\n",
    "                # 訓練時はバックプロパゲーション\n",
    "                if phase == \"train\"\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                end\n",
    "                # イテレーション結果の計算\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "                torch.set_grad_enabled(false)\n",
    "            end\n",
    "            \n",
    "            # epochごとの損失と正解率を表示\n",
    "            epoch_loss = epoch_loss / length(dataloaders[phase].dataset)\n",
    "            epoch_acc = epoch_corrects^2 / length(dataloaders[phase].dataset)\n",
    "            println(\"$(phase) Loss: $(epoch_loss), Acc: $(epoch_acc)\")\n",
    "        end\n",
    "        # 学習途中のモデルを保存\n",
    "        save(net, \"./data/03-1_model.pth\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RuntimeError(\"output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\") について\n",
    "これは、グレースケール画像が混ざっているために起こるエラーである\n",
    "\n",
    "本来は、グレースケールの画像を探し出して削除するのが良いのだが、面倒なので、画像読み込み時にRGB画像として読み込むように変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n",
      "valid Loss: 0.3077051222324371, Acc: PyObject tensor(63.0125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|████████████████████████▎                 | 11/19 [06:21<04:37, 34.71s/it]\u001b[A\n",
      "\n",
      " 33%|██████████████▋                             | 1/3 [00:01<00:02,  1.09s/it]\u001b[A\n",
      " 67%|█████████████████████████████▎              | 2/3 [00:09<00:05,  5.67s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████| 3/3 [00:18<00:00,  6.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "----------\n",
      "train Loss: 0.24474743228946996, Acc: PyObject tensor(476.5538)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████| 19/19 [06:51<00:00, 21.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.18694276362657547, Acc: PyObject tensor(70.3125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████| 3/3 [00:19<00:00,  6.38s/it]"
     ]
    }
   ],
   "source": [
    "# ハリネズミとヤマアラシのデータセット作成\n",
    "## ※ 画像をRGB画像として読み込む\n",
    "@pydef mutable struct Dataset <: torch.utils.data.Dataset\n",
    "    __init__(self, dir::AbstractString, phase::AbstractString=\"phase\") = begin\n",
    "        pybuiltin(:super)(Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.dir = dir\n",
    "        self.file_list = make_dataset_list(dir)\n",
    "    end\n",
    "    \n",
    "    __len__(self) = length(self.file_list)\n",
    "    \n",
    "    __getitem__(self, index::Int) = begin\n",
    "        # index番目の画像をロード\n",
    "        ## Juliaのindexは1〜なので +1 する\n",
    "        img_path = self.file_list[index + 1]\n",
    "        img = Image.open(img_path).convert(\"RGB\") # ←追加\n",
    "        img_transformed = image_transform_vgg16(img; phase=self.phase)\n",
    "        # 画像のラベル名をパスから抜き出す\n",
    "        label = img_path[length(self.dir) + 12 : length(self.dir) + 19]\n",
    "        # ハリネズミ: 0, ヤマアラシ: 1\n",
    "        label = (label == \"hedgehog\" ? 0 : 1)\n",
    "        return img_transformed, label\n",
    "    end\n",
    "end\n",
    "\n",
    "train_dataset = Dataset(\"train.noise\", \"train\")\n",
    "val_dataset = Dataset(\"valid\", \"valid\")\n",
    "\n",
    "# DataLoader作成\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset; batch_size=batch_size, shuffle=true\n",
    ")\n",
    "\n",
    "# 辞書にまとめる\n",
    "dataloaders = Dict(\n",
    "    \"train\" => train_dataloader,\n",
    "    \"valid\" => val_dataloader\n",
    ")\n",
    "\n",
    "# 学習・検証を実行\n",
    "train_model(net, dataloaders, criterion, optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float32,2}:\n",
       " -1.1368  1.13397"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 転移学習したモデルで改めてハリネズミ画像を認識させる\n",
    "\n",
    "net.eval() # 推論モードに設定\n",
    "\n",
    "# 画像読み込み\n",
    "image_file_path = \"./data/gahag-0059907781-1.jpg\"\n",
    "img = Image.open(image_file_path)\n",
    "\n",
    "# 画像をVGG16に読み込ませられるように処理する\n",
    "transform = make_transformer_for_vgg16()\n",
    "img_transformed = transform(img)\n",
    "\n",
    "# 転移学習したVGG-16モデルで予測実行\n",
    "pred = predict(net, [img_transformed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベルは `[ハリネズミ, ヤマアラシ]` と定義したため、上記の予測は `ヤマアラシ` という結果を表している\n",
    "\n",
    "したがって、今回の転移学習は失敗したということができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float32,2}:\n",
       " -1.31451  1.5244"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ヤマアラシの画像でも予測してみる\n",
    "img2 = Image.open(\"./data/publicdomainq-0025120muq.jpg\")\n",
    "img2_transformed = transform(img2)\n",
    "pred = predict(net, [img2_transformed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[], Any[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGG-16モデルを新規作成し、学習済モデルをロード\n",
    "net = models.vgg16()\n",
    "net.classifier[7] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "load(net, \"./data/03-1_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80-element Array{Any,1}:\n",
       "  true\n",
       "  true\n",
       " false\n",
       "  true\n",
       "  true\n",
       " false\n",
       " false\n",
       "  true\n",
       "  true\n",
       "  true\n",
       " false\n",
       "  true\n",
       "  true\n",
       "     ⋮\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true\n",
       "  true"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_list = []\n",
    "\n",
    "# 検証用データで推論実行\n",
    "for (image, label) in Dataset(\"valid\", \"valid\")\n",
    "    pred = predict(net, [image])\n",
    "    append!(correct_list, [(pred[1] < pred[2] && label == 1) || (pred[1] > pred[2] && label == 0)])\n",
    "end\n",
    "\n",
    "correct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 / 80: 正解率 90.0 %\n"
     ]
    }
   ],
   "source": [
    "# 正解数をカウント\n",
    "correct_count = length(correct_list[correct_list .== true])\n",
    "all_count = length(correct_list)\n",
    "println(\"$(correct_count) / $(all_count): 正解率 $(correct_count / all_count * 100) %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未知データの推論が上手く行かず、学習に使ったデータの推論の正解率が高いことから、過学習が起こっていると見込まれる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果と考察\n",
    "\n",
    "今回は、上手く転移学習させることができず、ハリネズミとヤマアラシを識別するモデルを作成することはできなかった\n",
    "\n",
    "この原因としては以下のようなものが考えられる\n",
    "\n",
    "1. ハリネズミとヤマアラシの教師データの数に差がありすぎた\n",
    "    - 以下のように、ヤマアラシの画像はハリネズミの画像の5倍近くあり、学習には不向きだった\n",
    "        - 訓練用画像数:\n",
    "            - ハリネズミ:  98枚\n",
    "            - ヤマアラシ: 487枚\n",
    "        - 検証用画像数:\n",
    "            - ハリネズミ: 40枚\n",
    "            - ヤマアラシ: 40枚\n",
    "2. 教師データそのものが誤っている可能性があった\n",
    "    - 人間が手動で分類しており、教師データそのものの妥当性が割と怪しかった\n",
    "3. 教師データ量が足りていなかった\n",
    "4. そもそもVGG-16モデル自体古いモデルであり、精度がそれほど高くない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
